/**
 * Ollama agent for local and cloud models
 * Supports Ollama API (local and cloud)
 * Documentation: https://docs.ollama.com/cloud
 */

export interface OllamaConfig {
  baseUrl?: string;
  apiKey?: string;
  model?: string;
}

export interface OllamaMessage {
  role: "system" | "user" | "assistant";
  content: string;
}

export interface OllamaRequest {
  model: string;
  messages: OllamaMessage[];
  stream?: boolean;
  options?: {
    temperature?: number;
    top_p?: number;
    max_tokens?: number;
  };
}

export interface OllamaResponse {
  model: string;
  created_at: string;
  message: {
    role: string;
    content: string;
  };
  done: boolean;
}

/**
 * Call Ollama API (local or cloud)
 */
export async function chatCompletion(
  request: OllamaRequest,
  config: OllamaConfig = {}
): Promise<OllamaResponse> {
  const baseUrl = config.baseUrl || process.env.OLLAMA_API_BASE || "http://localhost:11434";
  const apiKey = config.apiKey || process.env.OLLAMA_API_KEY;

  const headers: Record<string, string> = {
    "Content-Type": "application/json",
  };

  // Add Authorization header if API key is provided (for cloud)
  if (apiKey) {
    headers["Authorization"] = `Bearer ${apiKey}`;
  }

  const url = `${baseUrl}/api/chat`;

  const response = await fetch(url, {
    method: "POST",
    headers,
    body: JSON.stringify({
      model: request.model,
      messages: request.messages,
      stream: false,
      options: request.options,
    }),
  });

  if (!response.ok) {
    throw new Error(`Ollama API error: ${response.status} ${response.statusText}`);
  }

  return response.json() as Promise<OllamaResponse>;
}

/**
 * Ollama agent that follows the multi-agent protocol
 */
export async function agent(env: any): Promise<any> {
  const userText = env?.content?.text || "";
  const model = env?.inputs?.parameters?.model || env?.agent?.model || "llama3.2";
  const temperature = env?.inputs?.parameters?.temperature || 0.7;
  const maxTokens = env?.inputs?.parameters?.max_tokens || 2048;

  const messages: OllamaMessage[] = [
    {
      role: "system",
      content: "You are a helpful AI assistant powered by Ollama. Provide clear, accurate, and concise responses."
    },
    { role: "user", content: userText },
  ];

  const request: OllamaRequest = {
    model,
    messages,
    options: {
      temperature,
      max_tokens: maxTokens,
    },
  };

  const result = await chatCompletion(request);

  return {
    protocol: "multiagent-1.0",
    role: "agent",
    agent: {
      id: "ollama.agent",
      name: "Ollama Agent",
      model: result.model,
      version: "2025-10",
    },
    timestamp: new Date().toISOString(),
    intent: env.intent || "execute",
    task: env.task || "Process request",
    content: {
      type: "text",
      text: result.message.content,
    },
    outputs: {
      artifacts: [
        {
          kind: "response",
          name: "ollama_response",
          inline: result.message.content,
        },
      ],
    },
    confidence: {
      score: 0.85,
      rationale: `Response generated by ${result.model}`,
    },
  };
}

/**
 * List available models (local only)
 */
export async function listModels(config: OllamaConfig = {}): Promise<any> {
  const baseUrl = config.baseUrl || process.env.OLLAMA_API_BASE || "http://localhost:11434";

  const response = await fetch(`${baseUrl}/api/tags`);

  if (!response.ok) {
    throw new Error(`Failed to list models: ${response.status} ${response.statusText}`);
  }

  return response.json();
}
